{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLflow for Model Training, Tracking and Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset into a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/diabetes.csv\")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe API Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = data.dropna().select(col(\"Pregnancies\").astype(\"int\"),\n",
    "                           col(\"Glucose\").astype(\"int\"),\n",
    "                          col(\"BloodPressure\").astype(\"int\"),\n",
    "                          col(\"SkinThickness\").astype(\"int\"),\n",
    "                          col(\"Insulin\").astype(\"int\"),\n",
    "                          col(\"BMI\").astype(\"float\"),\n",
    "                          col(\"DiabetesPedigreeFunction\").astype(\"float\"),\n",
    "                          col(\"Age\").astype(\"int\"),\n",
    "                          col(\"Outcome\").astype(\"int\")\n",
    "                          )\n",
    "\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Our Model with Logging of Metrics and Pipeline Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diabetes_model(training_data, test_data, maxIterations, regularization):\n",
    "    import mlflow\n",
    "    import mlflow.spark\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    from mlflow.models.signature import infer_signature\n",
    "    import time\n",
    "    \n",
    "    # Start an MLflow run  \n",
    "    with mlflow.start_run():\n",
    "        numFeatures = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "\n",
    "        # define feature engineering and model steps\n",
    "\n",
    "        numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "        numScaler = MinMaxScaler(inputCol=numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "        featureVector = VectorAssembler(inputCols=[\"normalizedFeatures\"], outputCol=\"features\")\n",
    "        algo = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\", maxIter=maxIterations, regParam=regularization)\n",
    "\n",
    "        # chain the steps as stages in a Pipeline\n",
    "        Pipeline = Pipeline(stages=[numVector, numScaler,featureVector,algo])\n",
    "\n",
    "        # Log training parameter values\n",
    "        print (\"Training Logistic Regression model...\")\n",
    "        mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "        mlflow.log_param('regParam', algo.getRegParam())\n",
    "        model = Pipeline.fit(training_data)\n",
    "   \n",
    "        # Evaluate the model and log metrics\n",
    "        prediction = model.transform(test_data)\n",
    "        metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "        for metric in metrics:\n",
    "            evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=metric)\n",
    "            metricValue = evaluator.evaluate(prediction)\n",
    "            print(\"%s: %s\" % (metric, metricValue))\n",
    "            mlflow.log_metric(metric, metricValue)\n",
    "        \n",
    "        input_example = training_data.select(*numFeatures).limit(5).toPandas()\n",
    "        prediction_example = prediction.select(\"prediction\").limit(5).toPandas()\n",
    "\n",
    "        # Infer the signature\n",
    "        signature = infer_signature(input_example, prediction_example)\n",
    "   \n",
    "        # Log the model itself\n",
    "        unique_model_name = \"classifier-\" + str(time.time())\n",
    "        mlflow.spark.log_model(\n",
    "            model, \n",
    "            unique_model_name, \n",
    "            signature=signature, \n",
    "            input_example=input_example, \n",
    "            conda_env=mlflow.spark.get_default_conda_env()\n",
    "        )\n",
    "   \n",
    "        print(\"Experiment run complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Our Model with Specific Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diabetes_model(train, test, 5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diabetes_model(train, test, 10, 0.2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
