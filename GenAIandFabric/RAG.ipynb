{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) implementation with Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing synapseml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install synapseml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CSV Dataset into Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/diabetes_treatment_faq.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Sensitive Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the keys and endpoints in this area\n",
    "AZURE_OPENAI_KEY=\"YOUR_AZURE_OPENAU_RESOURCE_KEY\"\n",
    "AZURE_OPENAI_RESOURCE_NAME=\"AZURE_OPENAI_RESOURCE_NAME\"\n",
    "AI_SEARCH_NAME = \"YOUR_AI_SEARCH_RESOURCE_NAME\"\n",
    "AI_SEARCH_INDEX_NAME = \"YOUR_AZURE_AI_SEARCH_VECTOR_INDEX_NAME\"\n",
    "AI_SEARCH_API_KEY = \"YOUR_AZURE_AI_SEARCH_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Chunking the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.featurize.text import PageSplitter\n",
    "\n",
    "ps = (\n",
    "    PageSplitter()\n",
    "    .setInputCol(\"Description\")\n",
    "    .setMaximumPageLength(4000)\n",
    "    .setMinimumPageLength(30)\n",
    "    .setOutputCol(\"chunks\")\n",
    ")\n",
    "\n",
    "splitted_df = ps.transform(df)\n",
    "display(splitted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Each Chunk in a Seperate Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import posexplode, col, concat\n",
    "\n",
    "# Each \"chunks\" column contains the chunks for a single document in an array\n",
    "# The posexplode function will separate each chunk into its own row\n",
    "exploded_df = splitted_df.select(\"Topic\", posexplode(col(\"chunks\")).alias(\"chunk_index\", \"chunk\"))\n",
    "\n",
    "# Add a unique identifier for each chunk\n",
    "exploded_df = exploded_df.withColumn(\"unique_id\", concat(exploded_df.Topic, exploded_df.chunk_index))\n",
    "\n",
    "display(exploded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Embeddings for the Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.services import OpenAIEmbedding\n",
    "\n",
    "embedding = (\n",
    "    OpenAIEmbedding()\n",
    "    .setDeploymentName(\"text-embedding-ada-002\")  # Your Azure OpenAI deployment name\n",
    "    .setTextCol(\"chunk\")                          # Input column containing text\n",
    "    .setErrorCol(\"error\")                         # Column for logging errors\n",
    "    .setOutputCol(\"embeddings\")                   # Output column for embeddings\n",
    "    .setSubscriptionKey(f\"{AZURE_OPENAI_KEY}\")  # Replace with your Azure OpenAI key            # Specifies Azure OpenAI\n",
    "    .setEndpoint(f\"https://{AZURE_OPENAI_RESOURCE_NAME}.openai.azure.com/\")  # Azure OpenAI endpoint\n",
    ")\n",
    "\n",
    "df_embeddings = embedding.transform(exploded_df)\n",
    "\n",
    "display(df_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Vector Search Index in Azure AI Search Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Length of the embedding vector (OpenAI ada-002 generates embeddings of length 1536)\n",
    "EMBEDDING_LENGTH = 1536\n",
    "\n",
    "# Create index for AI Search with fields id, content, and contentVector\n",
    "# Note the datatypes for each field below\n",
    "url = f\"https://{AI_SEARCH_NAME}.search.windows.net/indexes/{AI_SEARCH_INDEX_NAME}?api-version=2023-11-01\"\n",
    "payload = json.dumps(\n",
    "    {\n",
    "        \"name\": AI_SEARCH_INDEX_NAME,\n",
    "        \"fields\": [\n",
    "            # Unique identifier for each document\n",
    "            {\n",
    "                \"name\": \"id\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"key\": True,\n",
    "                \"filterable\": True,\n",
    "            },\n",
    "            # Text content of the document\n",
    "            {\n",
    "                \"name\": \"content\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True,\n",
    "                \"retrievable\": True,\n",
    "            },\n",
    "            # Vector embedding of the text content\n",
    "            {\n",
    "                \"name\": \"contentVector\",\n",
    "                \"type\": \"Collection(Edm.Single)\",\n",
    "                \"searchable\": True,\n",
    "                \"retrievable\": True,\n",
    "                \"dimensions\": EMBEDDING_LENGTH,\n",
    "                \"vectorSearchProfile\": \"vectorConfig\",\n",
    "            },\n",
    "        ],\n",
    "        \"vectorSearch\": {\n",
    "            \"algorithms\": [{\"name\": \"hnswConfig\", \"kind\": \"hnsw\", \"hnswParameters\": {\"metric\": \"cosine\"}}],\n",
    "            \"profiles\": [{\"name\": \"vectorConfig\", \"algorithm\": \"hnswConfig\"}],\n",
    "        },\n",
    "    }\n",
    ")\n",
    "headers = {\"Content-Type\": \"application/json\", \"api-key\": AI_SEARCH_API_KEY}\n",
    "\n",
    "response = requests.request(\"PUT\", url, headers=headers, data=payload)\n",
    "if response.status_code == 201:\n",
    "    print(\"Index created!\")\n",
    "elif response.status_code == 204:\n",
    "    print(\"Index updated!\")\n",
    "else:\n",
    "    print(f\"HTTP request failed with status code {response.status_code}\")\n",
    "    print(f\"HTTP response body: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Documents and Text Chunks From Each Row with Vector Embeddings to Azure AI Search Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "\n",
    "def insert_into_index(documents):\n",
    "    \"\"\"Uploads a list of 'documents' to Azure AI Search index.\"\"\"\n",
    "\n",
    "    url = f\"https://{AI_SEARCH_NAME}.search.windows.net/indexes/{AI_SEARCH_INDEX_NAME}/docs/index?api-version=2023-11-01\"\n",
    "\n",
    "    payload = json.dumps({\"value\": documents})\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": AI_SEARCH_API_KEY,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.status_code == 200 or response.status_code == 201:\n",
    "        return \"Success\"\n",
    "    else:\n",
    "        return f\"Failure: {response.text}\"\n",
    "\n",
    "def make_safe_id(row_id: str):\n",
    "    \"\"\"Strips disallowed characters from row id for use as Azure AI search document ID.\"\"\"\n",
    "    return re.sub(\"[^0-9a-zA-Z_-]\", \"_\", row_id)\n",
    "\n",
    "\n",
    "def upload_rows(rows):\n",
    "    \"\"\"Uploads the rows in a Spark dataframe to Azure AI Search.\n",
    "    Limits uploads to 1000 rows at a time due to Azure AI Search API limits.\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = 1000\n",
    "    rows = list(rows)\n",
    "    for i in range(0, len(rows), BATCH_SIZE):\n",
    "        row_batch = rows[i : i + BATCH_SIZE]\n",
    "        documents = []\n",
    "        for row in rows:\n",
    "            documents.append(\n",
    "                {\n",
    "                    \"id\": make_safe_id(row[\"unique_id\"]),\n",
    "                    \"content\": row[\"chunk\"],\n",
    "                    \"contentVector\": row[\"embeddings\"].tolist(),\n",
    "                    \"@search.action\": \"upload\",\n",
    "                },\n",
    "            )\n",
    "        status = insert_into_index(documents)\n",
    "        yield [row_batch[0][\"row_index\"], row_batch[-1][\"row_index\"], status]\n",
    "\n",
    "# Add ID to help track what rows were successfully uploaded\n",
    "df_embeddings = df_embeddings.withColumn(\"row_index\", monotonically_increasing_id())\n",
    "\n",
    "# Run upload_batch on partitions of the dataframe\n",
    "res = df_embeddings.rdd.mapPartitions(upload_rows)\n",
    "display(res.toDF([\"start_index\", \"end_index\", \"insertion_status\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Function To Generate Vector Embeddings for the User Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_question_embedding(user_question):\n",
    "    \"\"\"Generates embedding for user_question using SynapseML.\"\"\"\n",
    "    from synapse.ml.services import OpenAIEmbedding\n",
    "\n",
    "    df_ques = spark.createDataFrame([(user_question, 1)], [\"questions\", \"what is diabetes?\"])\n",
    "    embedding = (\n",
    "        OpenAIEmbedding()\n",
    "        .setDeploymentName('text-embedding-ada-002')\n",
    "        .setTextCol(\"questions\")\n",
    "        .setErrorCol(\"errorQ\")\n",
    "        .setOutputCol(\"embeddings\")\n",
    "        .setSubscriptionKey(f\"{AZURE_OPENAI_KEY}\")  # Replace with your Azure OpenAI key            # Specifies Azure OpenAI\n",
    "        .setEndpoint(f\"https://{AZURE_OPENAI_RESOURCE_NAME}.openai.azure.com/\")\n",
    "    )\n",
    "    df_ques_embeddings = embedding.transform(df_ques)\n",
    "    row = df_ques_embeddings.collect()[0]\n",
    "    question_embedding = row.embeddings.tolist()\n",
    "    return question_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Top Chunks for the User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import requests\n",
    "\n",
    "def retrieve_top_chunks(k, question, question_embedding):\n",
    "    \"\"\"Retrieve the top K entries from Azure AI Search using hybrid search.\"\"\"\n",
    "    url = f\"https://{AI_SEARCH_NAME}.search.windows.net/indexes/{AI_SEARCH_INDEX_NAME}/docs/search?api-version=2023-11-01\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "        \"search\": question,\n",
    "        \"top\": k,\n",
    "        \"vectorQueries\": [\n",
    "            {\n",
    "                \"vector\": question_embedding,\n",
    "                \"k\": k,\n",
    "                \"fields\": \"contentVector\",\n",
    "                \"kind\": \"vector\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": AI_SEARCH_API_KEY,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    output = json.loads(response.text)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Context for Summarisation from the Chunks of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(user_question, retrieved_k = 5):\n",
    "    # Generate embeddings for the question\n",
    "    question_embedding = gen_question_embedding(user_question)\n",
    "\n",
    "    # Retrieve the top K entries\n",
    "    output = retrieve_top_chunks(retrieved_k, user_question, question_embedding)\n",
    "\n",
    "    # concatenate the content of the retrieved documents\n",
    "    context = [chunk[\"content\"] for chunk in output[\"value\"]]\n",
    "\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Response Generation from the GPT Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from synapse.ml.services.openai import OpenAIChatCompletion\n",
    "\n",
    "\n",
    "def make_message(role, content):\n",
    "    return Row(role=role, content=content, name=role)\n",
    "\n",
    "def get_response(user_question):\n",
    "    context = get_context(user_question)\n",
    "\n",
    "    # Write a prompt with context and user_question as variables \n",
    "    prompt = f\"\"\"\n",
    "    context: {context}\n",
    "    Answer the question based on the context above.\n",
    "    If the information to answer the question is not present in the given context then reply \"I don't know\".\n",
    "    \"\"\"\n",
    "\n",
    "    chat_df = spark.createDataFrame(\n",
    "        [\n",
    "            (\n",
    "                [\n",
    "                    make_message(\n",
    "                        \"system\", prompt\n",
    "                    ),\n",
    "                    make_message(\"user\", user_question),\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "    ).toDF(\"messages\")\n",
    "\n",
    "    chat_completion = (\n",
    "        OpenAIChatCompletion()\n",
    "        .setDeploymentName(\"gpt-4\") # deploymentName could be one of {gpt-35-turbo, gpt-35-turbo-16k}\n",
    "        .setMessagesCol(\"messages\")\n",
    "        .setErrorCol(\"error\")\n",
    "        .setOutputCol(\"chat_completions\")\n",
    "        .setSubscriptionKey(f\"{AZURE_OPENAI_KEY}\")  # Replace with your Azure OpenAI key            # Specifies Azure OpenAI\n",
    "        .setEndpoint(f\"https://{AZURE_OPENAI_RESOURCE_NAME}.openai.azure.com/\")\n",
    "\n",
    "    )\n",
    "\n",
    "    result_df = chat_completion.transform(chat_df).select(\"chat_completions.choices.message.content\")\n",
    "\n",
    "    result = []\n",
    "    for row in result_df.collect():\n",
    "        content_string = ' '.join(row['content'])\n",
    "        result.append(content_string)\n",
    "\n",
    "    # Join the list into a single string\n",
    "    result = ' '.join(result)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"how often should i check my blood sugar levels?\"\n",
    "response = get_response(user_question)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
