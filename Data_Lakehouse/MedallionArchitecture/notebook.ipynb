{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the csv file into a dataframe to read it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Files/Bronze/covid_data.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printing the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving bronze dataset as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"Files/Bronze/BronzeDataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the bronze dataset as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"BronzeTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating our silver layer dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "silver_df = spark.read.parquet(\"Files/Bronze/BronzeDataset.parquet\")\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting Columns in our silver layer dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "silver_df = silver_df.withColumn(\"Confirmed\", col(\"Confirmed\").cast(IntegerType()))\n",
    "silver_df = silver_df.withColumn(\"Deaths\", col(\"Deaths\").cast(IntegerType()))\n",
    "silver_df = silver_df.withColumn(\"Recovered\", col(\"Recovered\").cast(IntegerType()))\n",
    "silver_df = silver_df.withColumn(\"Last_Update\", to_date(col(\"Last_Update\"), \"M/d/yyyy\"))\n",
    "\n",
    "silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "storing silver dataframe as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "silver_df.write.mode(\"overwrite\").parquet(\"Files/Silver/SilverDataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "storing the silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "silver_df.write.mode(\"overwrite\").saveAsTable(\"SilverTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the silver layered data as a gold layered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gold_df = spark.read.parquet(\"Files/Silver/SilverDataset.parquet\")\n",
    "display(gold_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating aggregate values for year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, sum as F_sum\n",
    "\n",
    "# Extract the year from \"Last_Update\" and aggregate by year\n",
    "new_gold_df = (\n",
    "    gold_df\n",
    "    .select(\"Last_Update\", \"Country_Region\", \"Confirmed\", \"Deaths\", \"Recovered\")\n",
    "    .withColumn(\"Year\", year(\"Last_Update\"))  # Add a Year column\n",
    "    .groupBy(\"Year\")  # Group by the Year column\n",
    "    .agg(\n",
    "        F_sum(\"Confirmed\").alias(\"Total_Confirmed\"),\n",
    "        F_sum(\"Deaths\").alias(\"Total_Deaths\"),\n",
    "        F_sum(\"Recovered\").alias(\"Total_Recovered\")\n",
    "    )\n",
    "    .orderBy(\"Year\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "display(new_gold_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving gold data as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "new_gold_df.write.mode(\"overwrite\").parquet(\"Files/Gold/GoldDataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving this as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "new_gold_df.write.mode(\"overwrite\").saveAsTable(\"GoldTable\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
