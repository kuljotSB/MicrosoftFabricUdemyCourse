{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372795e6-01a9-4914-8c4f-93e94971bd1f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Spark session configuration\n",
    "This cell sets Spark session settings to enable _Verti-Parquet_ and _Optimize on Write_. More details about _Verti-Parquet_ and _Optimize on Write_ in tutorial document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a19297-6e9f-4020-937b-5d0ae7a10dd6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-07-19T18:02:48.728715Z",
       "execution_start_time": "2024-07-19T18:02:46.2233778Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7ca3fbc0-d822-455e-85a6-fa7bfec4dd7d",
       "queued_time": "2024-07-19T18:02:35.1114499Z",
       "session_id": "080a2fa1-6a46-4087-b392-f7536a0c8802",
       "session_start_time": "2024-07-19T18:02:35.3453281Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 080a2fa1-6a46-4087-b392-f7536a0c8802, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b25a84-a630-470a-9b52-a546214a1b86",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Fact - Sale\n",
    "\n",
    "This cell reads raw data from the _Files_ section of the lakehouse, adds additional columns for different date parts and the same information is being used to create partitioned fact delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23da9c7a-a5ef-413c-98c0-38d9d344f958",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-07-19T18:05:13.1549177Z",
       "execution_start_time": "2024-07-19T18:03:40.3888878Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "4daa6a66-b7e1-4424-93e9-a4b4b75d883f",
       "queued_time": "2024-07-19T18:03:39.9856435Z",
       "session_id": "080a2fa1-6a46-4087-b392-f7536a0c8802",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 080a2fa1-6a46-4087-b392-f7536a0c8802, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, month, quarter\n",
    "\n",
    "table_name = 'fact_sale'\n",
    "\n",
    "df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/WideWorldImportersDW/parquet/full/fact_sale_1y_full')\n",
    "df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\n",
    "df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\n",
    "df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140d7c6-c0c2-45b5-8038-9d062747e957",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Dimensions\n",
    "This cell creates a function to read raw data from the _Files_ section of the lakehouse for the table name passed as a parameter. Next, it creates a list of dimension tables. Finally, it has a _for loop_ to loop through the list of tables and call above function with each table name as parameter to read data for that specific table and create delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52fe7a3-01e5-4cfa-ac58-b7cb17b7153d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-07-19T18:06:32.5841133Z",
       "execution_start_time": "2024-07-19T18:05:57.7343426Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c13e809b-dba1-46b8-b684-c3f855511f0a",
       "queued_time": "2024-07-19T18:05:57.3175551Z",
       "session_id": "080a2fa1-6a46-4087-b392-f7536a0c8802",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 080a2fa1-6a46-4087-b392-f7536a0c8802, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def loadFullDataFromSource(table_name):\n",
    "    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/WideWorldImportersDW/parquet/full/' + table_name)\n",
    "    df = df.select([c for c in df.columns if c != 'Photo'])\n",
    "    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n",
    "\n",
    "full_tables = [\n",
    "    'dimension_city',\n",
    "    'dimension_customer',\n",
    "    'dimension_date',\n",
    "    'dimension_employee',\n",
    "    'dimension_stock_item'\n",
    "    ]\n",
    "\n",
    "for table in full_tables:\n",
    "    loadFullDataFromSource(table)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "2c52a91e-6ef4-4364-a525-33ceae21618a",
    "default_lakehouse_name": "wwilakehouse",
    "default_lakehouse_workspace_id": "953110e1-d237-4866-9e9a-f4278a2eefcd",
    "known_lakehouses": [
     {
      "id": "82e78e41-11a3-448e-a1aa-0bc48fc09cb6"
     },
     {
      "id": "2c52a91e-6ef4-4364-a525-33ceae21618a"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
