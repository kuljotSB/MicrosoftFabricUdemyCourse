{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset into a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"Tables/ecommerce_table\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing azure-ai-text-analytics SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-textanalytics==5.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting variables for our language resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_key=\"YOUR_LANGUAGE_RESOURCE_KEY\"\n",
    "language_endpoint=\"YOUR_LANGUAGE_RESOURCE_ENDPOINT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Language Analysis Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(language_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=language_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Sentiment Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(client, documents):\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "    \n",
    "    reviews=[]\n",
    "\n",
    "    for doc in doc_result:\n",
    "        reviews.append(doc.sentiment)\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to call sentiment analysis function in a Batch Size of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process reviews in batches\n",
    "def process_reviews_in_batches(reviews, batch_size=10):\n",
    "    results = []\n",
    "    for i in range(0, len(reviews), batch_size):\n",
    "        batch = reviews[i:i+batch_size]\n",
    "        sentiment_batch = sentiment_analysis(client, batch)  # Replace `None` with your client\n",
    "        results.extend(sentiment_batch)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Extraction and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reviews from the Spark DataFrame as a list\n",
    "reviews_list = [row[\"Review\"] for row in df.collect()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the sentiment analysis function by the process_reviews_in_batches function\n",
    "sentiments = process_reviews_in_batches(reviews_list, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Map sentiment results back to a DataFrame\n",
    "sentiments_rdd = spark.sparkContext.parallelize(zip(reviews_list, sentiments))\n",
    "sentiments_df = sentiments_rdd.toDF([\"review\", \"sentiment\"])\n",
    "\n",
    "# Join the sentiment DataFrame back to the original DataFrame\n",
    "df_with_sentiment = df.join(sentiments_df, \"review\")\n",
    "\n",
    "# Add \"positive\" and \"negative\" columns based on the sentiment\n",
    "df_with_sentiment = (\n",
    "    df_with_sentiment\n",
    "    .withColumn(\"positive\", (col(\"sentiment\") == \"positive\").cast(IntegerType()))\n",
    "    .withColumn(\"negative\", (col(\"sentiment\") == \"negative\").cast(IntegerType()))\n",
    ")\n",
    "\n",
    "# Show the final DataFrame with additional columns\n",
    "display(df_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates from the dataframe\n",
    "df_with_sentiment.dropDuplicates()\n",
    "display(df_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as a Parquet file\n",
    "df_with_sentiment.write.mode(\"overwrite\").parquet(\"Files/gold_layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as a table\n",
    "df_with_sentiment.write.mode(\"overwrite\").saveAsTable(\"gold_table\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
